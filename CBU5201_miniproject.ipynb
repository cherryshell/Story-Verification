{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# [Audio Story Authenticity Classification: A predictive model report based on Machine learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGn4ICrfqXZ"
   },
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**: Junying Lin  \n",
    "**Student ID**:  bupt:2022213482 qm:221169564\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o38VQkcdKd6k"
   },
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "The machine learning problem at hand is a binary classification task aimed at distinguishing between true stories and false stories based on audio features extracted from audio files. The interesting aspect of this problem lies in the challenge of accurately classifying narrative content using audio data, which involves understanding the nuances in speech patterns, tone, and other acoustic characteristics that may indicate truthfulness or deception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPTSuaB9L2jU"
   },
   "source": [
    "# 3 Methodology\n",
    "\n",
    "The methodology involves training and validating machine learning models to predict the class label ('True Story' or 'False Story') of audio files. Model performance is defined using accuracy, precision, recall, F1-score, and a confusion matrix. The training task involves feeding the models with audio features extracted from preprocessed audio files, while the validation task assesses the model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BwrtEdLDit"
   },
   "source": [
    "# 4 Implemented ML prediction pipelines\n",
    "\n",
    "The ML prediction pipelines implemented in this project involve three main stages: transformation, model training, and ensemble prediction. Each stage plays a critical role in the overall process of building and evaluating the machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1nDXnzYLLH6"
   },
   "source": [
    "## 4.1 Transformation stage\n",
    "\n",
    "The transformation stage involves preprocessing the audio files to extract relevant features that can be used for training machine learning models. The input is an audio file, and the output is a set of Mel-frequency cepstral coefficients (MFCCs) which are commonly used in speech recognition tasks. The choice of MFCCs is motivated by their ability to capture the timbral aspects of audio signals, which are crucial for distinguishing between different types of speech patterns.\n",
    "\n",
    "Input: Audio files in WAV format.  \n",
    "Output: A numerical array of MFCC features for each audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing file: CBU0521DD_stories\\00001.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00001.wav\n",
      "Processing file: CBU0521DD_stories\\00002.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00002.wav\n",
      "Processing file: CBU0521DD_stories\\00003.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00003.wav\n",
      "Processing file: CBU0521DD_stories\\00004.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00004.wav\n",
      "Processing file: CBU0521DD_stories\\00005.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00005.wav\n",
      "Processing file: CBU0521DD_stories\\00006.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00006.wav\n",
      "Processing file: CBU0521DD_stories\\00007.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00007.wav\n",
      "Processing file: CBU0521DD_stories\\00008.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00008.wav\n",
      "Processing file: CBU0521DD_stories\\00009.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00009.wav\n",
      "Processing file: CBU0521DD_stories\\00010.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00010.wav\n",
      "Processing file: CBU0521DD_stories\\00011.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00011.wav\n",
      "Processing file: CBU0521DD_stories\\00012.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00012.wav\n",
      "Processing file: CBU0521DD_stories\\00013.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00013.wav\n",
      "Processing file: CBU0521DD_stories\\00014.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00014.wav\n",
      "Processing file: CBU0521DD_stories\\00015.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00015.wav\n",
      "Processing file: CBU0521DD_stories\\00016.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00016.wav\n",
      "Processing file: CBU0521DD_stories\\00017.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00017.wav\n",
      "Processing file: CBU0521DD_stories\\00018.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00018.wav\n",
      "Processing file: CBU0521DD_stories\\00019.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00019.wav\n",
      "Processing file: CBU0521DD_stories\\00020.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00020.wav\n",
      "Processing file: CBU0521DD_stories\\00021.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00021.wav\n",
      "Processing file: CBU0521DD_stories\\00022.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00022.wav\n",
      "Processing file: CBU0521DD_stories\\00023.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00023.wav\n",
      "Processing file: CBU0521DD_stories\\00024.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00024.wav\n",
      "Processing file: CBU0521DD_stories\\00025.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00025.wav\n",
      "Processing file: CBU0521DD_stories\\00026.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00026.wav\n",
      "Processing file: CBU0521DD_stories\\00027.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00027.wav\n",
      "Processing file: CBU0521DD_stories\\00028.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00028.wav\n",
      "Processing file: CBU0521DD_stories\\00029.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00029.wav\n",
      "Processing file: CBU0521DD_stories\\00030.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00030.wav\n",
      "Processing file: CBU0521DD_stories\\00031.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00031.wav\n",
      "Processing file: CBU0521DD_stories\\00032.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00032.wav\n",
      "Processing file: CBU0521DD_stories\\00033.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00033.wav\n",
      "Processing file: CBU0521DD_stories\\00034.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00034.wav\n",
      "Processing file: CBU0521DD_stories\\00035.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00035.wav\n",
      "Processing file: CBU0521DD_stories\\00036.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00036.wav\n",
      "Processing file: CBU0521DD_stories\\00037.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00037.wav\n",
      "Processing file: CBU0521DD_stories\\00038.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00038.wav\n",
      "Processing file: CBU0521DD_stories\\00039.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00039.wav\n",
      "Processing file: CBU0521DD_stories\\00040.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00040.wav\n",
      "Processing file: CBU0521DD_stories\\00041.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00041.wav\n",
      "Processing file: CBU0521DD_stories\\00042.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00042.wav\n",
      "Processing file: CBU0521DD_stories\\00043.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00043.wav\n",
      "Processing file: CBU0521DD_stories\\00044.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00044.wav\n",
      "Processing file: CBU0521DD_stories\\00045.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00045.wav\n",
      "Processing file: CBU0521DD_stories\\00046.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00046.wav\n",
      "Processing file: CBU0521DD_stories\\00047.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00047.wav\n",
      "Processing file: CBU0521DD_stories\\00048.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00048.wav\n",
      "Processing file: CBU0521DD_stories\\00049.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00049.wav\n",
      "Processing file: CBU0521DD_stories\\00050.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00050.wav\n",
      "Processing file: CBU0521DD_stories\\00051.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00051.wav\n",
      "Processing file: CBU0521DD_stories\\00052.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00052.wav\n",
      "Processing file: CBU0521DD_stories\\00053.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00053.wav\n",
      "Processing file: CBU0521DD_stories\\00054.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00054.wav\n",
      "Processing file: CBU0521DD_stories\\00055.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00055.wav\n",
      "Processing file: CBU0521DD_stories\\00056.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00056.wav\n",
      "Processing file: CBU0521DD_stories\\00057.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00057.wav\n",
      "Processing file: CBU0521DD_stories\\00058.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00058.wav\n",
      "Processing file: CBU0521DD_stories\\00059.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00059.wav\n",
      "Processing file: CBU0521DD_stories\\00060.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00060.wav\n",
      "Processing file: CBU0521DD_stories\\00061.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00061.wav\n",
      "Processing file: CBU0521DD_stories\\00062.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00062.wav\n",
      "Processing file: CBU0521DD_stories\\00063.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00063.wav\n",
      "Processing file: CBU0521DD_stories\\00064.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00064.wav\n",
      "Processing file: CBU0521DD_stories\\00065.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00065.wav\n",
      "Processing file: CBU0521DD_stories\\00066.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00066.wav\n",
      "Processing file: CBU0521DD_stories\\00067.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00067.wav\n",
      "Processing file: CBU0521DD_stories\\00068.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00068.wav\n",
      "Processing file: CBU0521DD_stories\\00069.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00069.wav\n",
      "Processing file: CBU0521DD_stories\\00070.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00070.wav\n",
      "Processing file: CBU0521DD_stories\\00071.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00071.wav\n",
      "Processing file: CBU0521DD_stories\\00072.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00072.wav\n",
      "Processing file: CBU0521DD_stories\\00073.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00073.wav\n",
      "Processing file: CBU0521DD_stories\\00074.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00074.wav\n",
      "Processing file: CBU0521DD_stories\\00075.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00075.wav\n",
      "Processing file: CBU0521DD_stories\\00076.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00076.wav\n",
      "Processing file: CBU0521DD_stories\\00077.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00077.wav\n",
      "Processing file: CBU0521DD_stories\\00078.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00078.wav\n",
      "Processing file: CBU0521DD_stories\\00079.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00079.wav\n",
      "Processing file: CBU0521DD_stories\\00080.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00080.wav\n",
      "Processing file: CBU0521DD_stories\\00081.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00081.wav\n",
      "Processing file: CBU0521DD_stories\\00082.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00082.wav\n",
      "Processing file: CBU0521DD_stories\\00083.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00083.wav\n",
      "Processing file: CBU0521DD_stories\\00084.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00084.wav\n",
      "Processing file: CBU0521DD_stories\\00085.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00085.wav\n",
      "Processing file: CBU0521DD_stories\\00086.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00086.wav\n",
      "Processing file: CBU0521DD_stories\\00087.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00087.wav\n",
      "Processing file: CBU0521DD_stories\\00088.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00088.wav\n",
      "Processing file: CBU0521DD_stories\\00089.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00089.wav\n",
      "Processing file: CBU0521DD_stories\\00090.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00090.wav\n",
      "Processing file: CBU0521DD_stories\\00091.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00091.wav\n",
      "Processing file: CBU0521DD_stories\\00092.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00092.wav\n",
      "Processing file: CBU0521DD_stories\\00093.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00093.wav\n",
      "Processing file: CBU0521DD_stories\\00094.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00094.wav\n",
      "Processing file: CBU0521DD_stories\\00095.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00095.wav\n",
      "Processing file: CBU0521DD_stories\\00096.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00096.wav\n",
      "Processing file: CBU0521DD_stories\\00097.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00097.wav\n",
      "Processing file: CBU0521DD_stories\\00098.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00098.wav\n",
      "Processing file: CBU0521DD_stories\\00099.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00099.wav\n",
      "Processing file: CBU0521DD_stories\\00100.wav\n",
      "Preprocessing audio file: CBU0521DD_stories\\00100.wav\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Function to preprocess an audio file\n",
    "def preprocess_audio(file_path):\n",
    "    print(\"Preprocessing audio file:\", file_path)\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "    audio_data, _ = librosa.load(file_path, sr=16000)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=16000, n_mfcc=13)\n",
    "    mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs_processed\n",
    "\n",
    "# Function to load data\n",
    "def load_data(audio_dir, csv_file):\n",
    "    print(\"Loading data...\")\n",
    "    attributes = pd.read_csv(csv_file)\n",
    "    audio_files = [os.path.join(audio_dir, f\"{idx:05d}.wav\") for idx in range(1, 101)]\n",
    "    data = {\n",
    "        \"audio_features\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    for file in audio_files:\n",
    "        filename = os.path.basename(file)\n",
    "        story = attributes[attributes['filename'] == filename]\n",
    "        story_type = story['Story_type'].values[0]\n",
    "        print(\"Processing file:\", file)\n",
    "        data['audio_features'].append(preprocess_audio(file))\n",
    "        data['labels'].append(1 if story_type == 'True Story' else 0)\n",
    "    print(\"Data loaded.\")\n",
    "    return data\n",
    "\n",
    "# Main function to load and return data\n",
    "def main():\n",
    "    audio_dir = 'CBU0521DD_stories'\n",
    "    csv_file = 'CBU0521DD_stories_attributes.csv'\n",
    "    data = load_data(audio_dir, csv_file)\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F5_kI95LuZ2"
   },
   "source": [
    "## 4.2 Model stage\n",
    "\n",
    "The model stage involves the creation and training of two different machine learning models: a Multi-Layer Perceptron (MLP) and a Random Forest Classifier.  \n",
    "\n",
    "**MLP Model:**  \n",
    "Architecture: The MLP consists of three fully connected layers with ReLU activations.  \n",
    "Input: MFCC feature vectors.  \n",
    "Output: A probability distribution over two classes (true story or false story).  \n",
    "Rationale: MLPs are chosen for their ability to model complex, non-linear relationships in the data. They are suitable for capturing the intricate patterns in speech that may be indicative of deception or truth.  \n",
    "  \n",
    "  \n",
    "**Random Forest Classifier:**  \n",
    "Architecture: An ensemble of decision trees.  \n",
    "Input: MFCC feature vectors.  \n",
    "Output: A class label (true story or false story).  \n",
    "Rationale: Random Forests are chosen for their robustness to overfitting and their ability to handle high-dimensional data. They provide a good baseline for comparison with other models due to their versatility and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cpu\n",
      "Training MLP model...\n",
      "Epoch 1, Loss: 4.426042079925537\n",
      "Epoch 2, Loss: 1.0982061624526978\n",
      "Epoch 3, Loss: 1.663364052772522\n",
      "Epoch 4, Loss: 0.8481653332710266\n",
      "Epoch 5, Loss: 1.2898378372192383\n",
      "Epoch 6, Loss: 0.9568106532096863\n",
      "Epoch 7, Loss: 0.8622147440910339\n",
      "Epoch 8, Loss: 1.0425703525543213\n",
      "Epoch 9, Loss: 0.7478941082954407\n",
      "Epoch 10, Loss: 0.8406487703323364\n",
      "MLP model training complete.\n",
      "Training Random Forest model...\n",
      "Random Forest model training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device set to:\", device)\n",
    "\n",
    "# Define a simple Multi-Layer Perceptron (MLP) model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the MLP model\n",
    "def train_mlp_model(features, labels):\n",
    "    print(\"Training MLP model...\")\n",
    "    features_array = np.array(features)\n",
    "    features_tensor = torch.tensor(features_array, dtype=torch.float32).to(device)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    model = MLP(features_tensor.shape[1]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(10):  # Increase the number of training epochs to improve accuracy\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features_tensor)\n",
    "        loss = criterion(outputs, labels_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    print(\"MLP model training complete.\")\n",
    "    return model\n",
    "\n",
    "# Function to train the Random Forest model\n",
    "def train_random_forest(audio_features, labels):\n",
    "    print(\"Training Random Forest model...\")\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    clf.fit(audio_features, labels)\n",
    "    print(\"Random Forest model training complete.\")\n",
    "    return clf\n",
    "\n",
    "# Function to split data into training and testing sets, and train both models\n",
    "def train_models(audio_features, labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(audio_features, labels, test_size=0.2, random_state=42)\n",
    "    mlp_model = train_mlp_model(X_train, y_train)\n",
    "    rf_model = train_random_forest(X_train, y_train)\n",
    "    return mlp_model, rf_model, X_test, y_test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming 'data' is the data loaded from the first cell\n",
    "    mlp_model, rf_model, X_test, y_test = train_models(data['audio_features'], np.array(data['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Ensemble stage\n",
    "\n",
    "The ensemble stage combines the predictions of the MLP and the Random Forest Classifier to produce a final prediction.\n",
    "\n",
    "**Voting System:**\n",
    "If the two models agree on a prediction, that prediction is chosen.  \n",
    "If there is a disagreement, a random choice is made between the two classes with equal probability.\n",
    "\n",
    "Rationale:  \n",
    "Ensemble methods are used to improve the overall performance of the model by combining the strengths of individual models. The random choice in case of a tie is a simple way to break ties without biasing towards any class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.65\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.78      0.67         9\n",
      "           1       0.75      0.55      0.63        11\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.67      0.66      0.65        20\n",
      "weighted avg       0.68      0.65      0.65        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function for ensemble prediction using both models\n",
    "def ensemble_predict(mlp_model, rf_model, audio_features):\n",
    "    # Convert audio features to a tensor\n",
    "    audio_features_tensor = torch.tensor(np.array(audio_features), dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Use the MLP model for prediction\n",
    "    mlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        mlp_outputs = mlp_model(audio_features_tensor)\n",
    "    mlp_predictions = mlp_outputs.argmax(dim=1).cpu().numpy()\n",
    "    \n",
    "    # Use the Random Forest model for prediction\n",
    "    rf_predictions = rf_model.predict(audio_features)\n",
    "    \n",
    "    # Voting system: if both models agree on a prediction, use that result; otherwise, randomly choose between the two\n",
    "    ensemble_predictions = np.where(mlp_predictions == rf_predictions, mlp_predictions, np.random.choice([0, 1], p=[0.5, 0.5]))\n",
    "    \n",
    "    return ensemble_predictions\n",
    "\n",
    "# Function to evaluate the ensemble model and print accuracy and classification report\n",
    "def evaluate_ensemble_model(mlp_model, rf_model, X_test, y_test):\n",
    "    ensemble_predictions = ensemble_predict(mlp_model, rf_model, X_test)\n",
    "    ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "    print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "    # Add the zero_division parameter to control the behavior when precision is ill-defined\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, ensemble_predictions, zero_division=0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming 'data' is the data loaded from the first cell\n",
    "    # Assuming 'mlp_model' and 'rf_model' are the models trained in the second cell\n",
    "    evaluate_ensemble_model(mlp_model, rf_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQPxztuL9AW"
   },
   "source": [
    "# 5 Dataset\n",
    "\n",
    "The dataset is based on the MLEnd Deception Dataset, which consists of audio files labeled as either true stories or false stories. The dataset is constructed by loading the audio files and their corresponding labels from a CSV file. The dataset is then split into training and testing sets, with 80% of the data used for training and 20% for testing. This split ensures that the models are evaluated on independent and identically distributed (IID) samples, which is crucial for assessing their generalization capabilities.\n",
    "\n",
    "Exploration of the dataset involves examining the distribution of the classes and visualizing the MFCC features to understand their variability across different audio files. This step is essential for identifying any potential biases or imbalances in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf7GN1aeXJI"
   },
   "source": [
    "# 6 Experiments and results\n",
    "\n",
    "The experiments involve training the MLP and Random Forest models on the training set and evaluating their performance on the testing set. The results show an ensemble accuracy of 0.65, with precision, recall, and F1-score values indicating room for improvement, particularly for the false story class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSrJCR_cekPO"
   },
   "source": [
    "# 7 Conclusions\n",
    "\n",
    "The final performance of the ensemble model indicates that while some level of accuracy has been achieved, there is significant room for improvement. The classification report highlights the need for better recall and precision, especially for the false story class. Suggestions for improvements include exploring more sophisticated feature extraction techniques, experimenting with different model architectures, and employing more advanced ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 References\n",
    "\n",
    "The implementation of the models and the methodology followed in this project were informed by various resources, including textbooks on machine learning, online tutorials on audio processing with Python, and documentation for the libraries used (such as PyTorch, scikit-learn, and librosa). Specific references include the following:\n",
    "\n",
    "Librosa: Audio and Music Analysis in Python\n",
    "PyTorch: An Imperative Style, High-Performance Deep Learning Library\n",
    "Scikit-learn: Machine Learning in Python"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
